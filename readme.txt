Steps to run the code:

TrainingScript:
Step 1: Installing and importing dependencies
	Run code cell 1 and code cell 2 (in order)
Step 2: Defining the similarity model architecture class and forward propagation
	Run code cell 3
Step 3: Defining the contrastive loss function
	Run code cell 4
Step 4: Checking for GPU availability 
	Run code cell 5
Step 5: Loading the gsarti/biobert-nli pretrained model
	Run code cell 6
Step 6: Loading the training and test datasets
	Run code cell 7
Step 7: Defining the utility functions required to extract word embeddings from pretrained model
	Run code cell 8, code cell 9 and code cell 10 (in order)
Step 8: 2 options-
	1. Extracting word embeddings for training set records
	   Run code cell 12 (to extract word embeddings) and code cell 13 (to save the calculated word embedding in a .pt file) (in order)
	2. Loading previously saved word embeddings for training set records
	   Run code cell 11
Step 9: 2 options-
	1. Extracting word embeddings for test set records
	   Run code cell 15 (to extract word embeddings) and code cell 16 (to save the calculated word embedding in a .pt file) (in order)
	2. Loading previously saved word embeddings for test set records
	   Run code cell 14
Step 10: Defining the similarity model parameters
	 Run code cell 17
Step 11: 2 options-
	 1. Initializing a similarity model instance
	    Run code cell 18
	 2. Loading trained similarity model
	    Run code cell 21
Step 12: Training the model
	 Run code cell 19
Step 13: Saving parameters of trained similarity model
	 Run code cell 21
Step 14: Calculating Training set accuracy
	 Run code cell 22
Step 15: Calculating Test set accuracy
	 Run code cell 23

SimilarityModel (Driver script):

Step 1: Installing and importing dependencies
	Run code cell 1 and code cell 2 (in order)
Step 2: Defining the similarity model architecture class and forward propagation
	Run code cell 3
Step 3: Checking for GPU availability 
	Run code cell 4
Step 4: Loading the gsarti/biobert-nli pretrained model
	Run code cell 5
Step 5: Defining the similarity model parameters
	Run code cell 6
Step 6: Loading trained similarity model
	Run code cell 7
Step 7: Loading the question-answer database
	Run code cell 8
Step 8: Defining the utility functions required to extract the best match to the new query from the database using the similarity model 
	Run code cell 9, code cell 10 and code cell 11 (in order)
Step 9: Demo driver code to demonstrate the working
 	Run code cell 12


About the model:

The similarity model makes use of pretrained word embeddings gsarti/biobert-nli(link: https://huggingface.co/gsarti/biobert-nli?text=The+goal+of+life+is+%5BMASK%5D.) and fine tunes it with the training data to generate meaninful sentence embeddings for biomedical semantic similarity analysis.
Other pretrained word embeddings model that performed well on the training set are BioBERT (Implementation details here: https://www.kaggle.com/doggydev/biobert-embeddings-demo)

The pretrained model is finetuned by adding a stacked BiLSTM layer(2 BiLSTMs) and a fully connected layer at the end of the pretrained model. 
The BiLSTMs are used to extract meaningful sentence embeddings from the input word embeddings.
The model takes as input the word embeddings for the sentence and generates embedding for the sentence as a whole.
The hidden layer of each LSTM unit is equal to the length of embedding of each word(here 768). 
The BiLSTMs are able to store important context from different parts of the sentence and generate a more semantically accurate embedding.
The outputs of the last forward LSTM unit and backward LSTM unit are concatenated and fed into a fully connected layer which generates the final sentence embeddings. 
A better model would be generated by stacking 10 BiLSTM layers since the memorizing ability of this model is unmatched by a 2 layer BiLSTM stack, however due to the limited computing resources such a model could not be trained.

The model was trained as a siamese network with the loss criterion as contrastive loss and optimizer as the adam optimizer.
Training set consists of 10486 sentence pairs and the test set consists of 2080 sentence pairs.
Both the training set and test set have been borrowed from various repositories of Asma Ben Abacha(Link: https://github.com/abachaa/).

The model performs extremely well on the training as well as the test set,
Accuracy on training set: 0.9673850848750716
Accuracy on test set: 0.9692307692307692

However, since the training set and test set are drawn from similar sources it might be overfit to data and require training on the question answer database. A little overfitting to the question answer database is infact desirable since accurate matching to questions in the database is desired.

The question with the closest semantic relationship to the new query entered by the user is considered to be the one having the minimum pairwise euclidean distance between their sentence embeddings.

  